{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # MVPA Full 9 classes\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths\n",
    "root_dir = '/Volumes/T7/BIDS-BRAINPLAYBACK-TASK2'\n",
    "fmriprep_dir = os.path.join(root_dir, 'derivatives', 'fmriprep23')\n",
    "dataset_dir  = os.path.join(root_dir, 'derivatives', 'mvpa_06_full_bold', 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list datasets and concatenate\n",
    "\n",
    "# find all *_dataset.nii.gz files in dataset_dir\n",
    "dataset_files = [os.path.join(dataset_dir, f) for f in os.listdir(dataset_dir) if f.endswith('_features.npy') & f.startswith('sub-')]\n",
    "dataset_files.sort()\n",
    "\n",
    "# find all *_trial_types.txt files in dataset_dir\n",
    "label_files = [os.path.join(dataset_dir, f) for f in os.listdir(dataset_dir) if f.endswith('_labels.npy') & f.startswith('sub-')]\n",
    "label_files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subjects found: 17.000000\n"
     ]
    }
   ],
   "source": [
    "# get number of subjects\n",
    "n_runs_per_sub = 4\n",
    "n_subjects = len(dataset_files) / n_runs_per_sub\n",
    "print('Number of subjects found: %f' % n_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of rows: 4896.0\n"
     ]
    }
   ],
   "source": [
    "# estimate expected number of rows\n",
    "# 17 subjects, 8 noise and 9 x 2 music for each of the 4 runs\n",
    "\n",
    "n_noise_trials = 0\n",
    "n_noise_splits = 0\n",
    "n_music_trials = 9*2\n",
    "n_music_splits = 4\n",
    "\n",
    "n_rows_estimate_per_sub = n_runs_per_sub * (n_noise_trials*n_noise_splits + n_music_trials*n_music_splits)\n",
    "n_rows_estimate = n_subjects * n_rows_estimate_per_sub\n",
    "print(f'Estimated number of rows: {n_rows_estimate}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating datasets...\n",
      "Dataset shape: (4896, 2671)\n"
     ]
    }
   ],
   "source": [
    "# concatenate all datasets in dataset_files\n",
    "print('Concatenating datasets...')\n",
    "samples = np.concatenate([np.load(f) for f in dataset_files], axis=0)\n",
    "print('Dataset shape: %s' % str(samples.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating labels...\n",
      "Labels shape: (4896,)\n"
     ]
    }
   ],
   "source": [
    "# concatenate all labels into a single string array\n",
    "print('Concatenating labels...')\n",
    "labels = np.concatenate([np.load(f, allow_pickle=True) for f in label_files], axis=0)\n",
    "print('Labels shape: %s' % str(labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 9\n"
     ]
    }
   ],
   "source": [
    "class_names = np.unique(labels)\n",
    "n_classes = len(class_names)\n",
    "print(f'Number of classes: {n_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate an array of chunk labels\n",
    "chunks = np.repeat(np.arange(1,n_subjects+1), n_rows_estimate_per_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4896,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Leave One Subject Out Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1/17...fold 2/17...\n",
      "\n",
      "fold 3/17...\n",
      "fold 4/17...\n",
      "fold 5/17...\n",
      "fold 6/17...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "n_folds = int(n_subjects)\n",
    "acc_array = np.zeros(n_folds)\n",
    "acc_bal_array = np.zeros(n_folds)\n",
    "confusion_matrix_array = np.zeros((n_classes,n_classes,n_folds))\n",
    "\n",
    "def process_fold(ff):\n",
    "    print(f'fold {ff+1}/{n_folds}...')\n",
    "\n",
    "    clf = LinearSVC(multi_class=\"ovr\", max_iter=5000, class_weight='balanced', dual='auto')\n",
    "    \n",
    "    # split the data into training and test set\n",
    "    train_mask = chunks != ff+1\n",
    "    test_mask = chunks == ff+1\n",
    "\n",
    "    X_train = samples[train_mask, :]\n",
    "    y_train = labels[train_mask]\n",
    "    X_test = samples[test_mask, :]\n",
    "    y_test = labels[test_mask]\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Compute the prediction accuracy for the different labels\n",
    "    acc = (y_pred == y_test).mean()\n",
    "    acc_bal = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "\n",
    "    return ff, acc, acc_bal, cm\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=6) as executor:\n",
    "    results = list(executor.map(process_fold, range(n_folds)))\n",
    "\n",
    "# Update the accuracy arrays with the results\n",
    "for ff, acc, acc_bal, cm in results:\n",
    "    acc_array[ff] = acc\n",
    "    acc_bal_array[ff] = acc_bal\n",
    "    confusion_matrix_array[...,ff] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 11.2% ± 2.1%\n",
      "Mean balanced accuracy: 11.2% ± 2.1%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean accuracy: {acc_array.mean()*100:0.1f}% \\u00B1 {acc_array.std()*100:0.1f}%\")\n",
    "print(f\"Mean balanced accuracy: {acc_bal_array.mean()*100:0.1f}% \\u00B1 {acc_bal_array.std()*100:0.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.52941176, 3.05882353, 3.70588235, 4.23529412, 3.94117647,\n",
       "        3.52941176, 3.82352941, 3.        , 3.17647059],\n",
       "       [4.        , 2.88235294, 4.35294118, 3.58823529, 3.52941176,\n",
       "        3.17647059, 3.88235294, 3.29411765, 3.29411765],\n",
       "       [3.94117647, 3.11764706, 4.05882353, 3.11764706, 3.94117647,\n",
       "        3.29411765, 3.52941176, 3.35294118, 3.64705882],\n",
       "       [3.52941176, 4.        , 4.17647059, 4.29411765, 2.76470588,\n",
       "        3.35294118, 3.        , 3.17647059, 3.70588235],\n",
       "       [3.35294118, 3.88235294, 4.05882353, 2.88235294, 4.11764706,\n",
       "        3.52941176, 3.23529412, 3.64705882, 3.29411765],\n",
       "       [3.94117647, 3.76470588, 3.70588235, 3.64705882, 3.64705882,\n",
       "        2.64705882, 3.82352941, 3.35294118, 3.47058824],\n",
       "       [3.41176471, 3.29411765, 4.05882353, 3.29411765, 4.35294118,\n",
       "        3.11764706, 3.76470588, 3.64705882, 3.05882353],\n",
       "       [2.82352941, 4.35294118, 3.76470588, 3.64705882, 3.94117647,\n",
       "        2.88235294, 3.52941176, 3.70588235, 3.35294118],\n",
       "       [3.11764706, 3.76470588, 4.35294118, 3.58823529, 4.        ,\n",
       "        3.76470588, 3.76470588, 2.52941176, 3.11764706]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = np.mean(confusion_matrix_array, 2)*100\n",
    "M_annot = []\n",
    "\n",
    "for i in range(M.shape[0]):\n",
    "    for j in range(M.shape[1]):\n",
    "        aux = f'{M[i, j]:0.1f}%'\n",
    "        M_annot.append(aux)\n",
    "\n",
    "M_annot_plot =  np.array(M_annot).reshape(M.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(M, annot=M_annot_plot, fmt = '', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names, \n",
    "            annot_kws={\"size\": 14},\n",
    "            cbar_kws={'label': 'Percentage of true labels'})\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title(f'Overall balanced accuracy = {acc_array.mean()*100:0.1f}% \\u00B1 {acc_array.std()*100:0.1f}%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "acc_array_dummy = np.zeros(n_folds)\n",
    "acc_bal_array_dummy = np.zeros(n_folds)\n",
    "confusion_matrix_array_dummy = np.zeros((n_classes,n_classes,n_folds))\n",
    "\n",
    "def process_fold_dummy(ff):\n",
    "    print(f'fold {ff+1}/{n_folds}...')\n",
    "\n",
    "    clf_dummy = DummyClassifier(strategy='prior')\n",
    "    \n",
    "    # split the data into training and test set\n",
    "    train_mask = chunks != ff+1\n",
    "    test_mask = chunks == ff+1\n",
    "\n",
    "    X_train = samples[train_mask, :]\n",
    "    y_train = labels[train_mask]\n",
    "    X_test = samples[test_mask, :]\n",
    "    y_test = labels[test_mask]\n",
    "\n",
    "    clf_dummy.fit(X_train, y_train)\n",
    "    y_pred = clf_dummy.predict(X_test)\n",
    "\n",
    "    # Compute the prediction accuracy for the different labels\n",
    "    acc = (y_pred == y_test).mean()\n",
    "    acc_bal = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return ff, acc, acc_bal, cm\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=6) as executor:\n",
    "    results = list(executor.map(process_fold_dummy, range(n_folds)))\n",
    "\n",
    "# Update the accuracy arrays with the results\n",
    "for ff, acc, acc_bal, cm in results:\n",
    "    acc_array_dummy[ff] = acc\n",
    "    acc_bal_array_dummy[ff] = acc_bal\n",
    "    confusion_matrix_array_dummy[...,ff] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean accuracy: {acc_array_dummy.mean()*100:0.1f}% \\u00B1 {acc_array_dummy.std()*100:0.1f}%\")\n",
    "print(f\"Mean balanced accuracy: {acc_bal_array_dummy.mean()*100:0.1f}% \\u00B1 {acc_bal_array_dummy.std()*100:0.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Linear SVC vs. Dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mann-Whitney U Test Statistic: 212.5\n",
      "P-value: 0.011765827200792207\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# Perform Wilcoxon Signed-Rank Test\n",
    "stat, p_value = mannwhitneyu(acc_bal_array, acc_bal_array_dummy)\n",
    "\n",
    "print(f'Mann-Whitney U Test Statistic: {stat}')\n",
    "print(f'P-value: {p_value}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainplayback_task02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
